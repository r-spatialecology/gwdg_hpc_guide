---
title: "GWDG HPC GUIDE"
author: 
- Marco Sciaini
- Maximilian Hesselbarth
output:
  prettydoc::html_pretty:
    toc: true
    toc_depth: 2
    theme: cayman
    highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## GWDG HPC

This is meant to be a short introduction how you can use the GWDG HPC  (for account-holders only), specifically for R user.

The aim of this guide is to enable you to use your local R session and submit jobs from your office to the GWDG HPC and afterwards retrieve the results also in your local session.
This has the huge advantage, that you don't need to copy data from your machine to the HPC (and vice versa) and working in your IDE is most likely way more efficient than working in 
linux shell via ssh on the HPC ...

The structure of this guide is heavily influenced by our own mistakes and we hope to make it somewhat easier for future HPC user.
In general, the our setup should also work with any other HPC and the code snippets in the end are also usable for our scheduling systems than LSF (which is used be the GWDG).

Huge thanks to the GWDG HPC team that put quite some effort into installing R trifles and explaining some HPC basics to us.
Another big thanks to [https://twitter.com/henrikbengtsson?lang=en](Henrik Bengtsson), [https://twitter.com/_ms03?lang=en](Michael Schubert) and [https://github.com/mllg](Michel Lang) for developing such nice tools (future, clustermq, batchtools) to enable even ecologists to use cluster infrastrace ;-)

There are some prelimanry steps you have to take, before you can actually use the HPC these include:

### 1. SSH Key
For a convienent login and to use R via SSH connector, you need to put a SSH key on the HPC (private key on your computer, public key on the HPC).

Assuming you are using linux or mac, that is rather straighforward:

```{bash eval = FALSE}
ssh-keygen -t rsa
```

and copy it to the HPC:

```{bash eval = FALSE}
ssh-copy-id gwdg_user@gwdu101.gwdg.de
```

... I think copy is not installed by default on mac, so you first have to install it:
```{bash eval = FALSE}
brew install ssh-copy-id
```

If you are using Windows, this guide seems to cover it:

[https://docs.joyent.com/public-cloud/getting-started/ssh-keys/generating-an-ssh-key-manually/manually-generating-your-ssh-key-in-windows](https://docs.joyent.com/public-cloud/getting-started/ssh-keys/generating-an-ssh-key-manually/manually-generating-your-ssh-key-in-windows)

### 2. Create .bashrc

Login to the HPC via:

```{bash eval = FALSE}
ssh gwdg_user@gwdu101.gwdg.de
```

and create the file `.bashrc`:

```{bash eval = FALSE}
nano .bashrc
```

Copy and paste the following in it:

```{bash eval = FALSE}
# .bashrc
# Source global definitions
if [ -f /etc/bashrc ]; then
    . /etc/bashrc
fi
# Uncomment the following line if you don't like systemctl's auto-paging feature:
# export SYSTEMD_PAGER=
# User specific aliases and functions

module load gcc/6.3.0
module load R
module load zeromq/4.2.3

source /etc/profile.d/lsf.sh
```

Environment `modules` (used via the command `module` in the above snippet) provide a way to selectively activate and deactivate modifications to the user environment which allow particular packages and versions to be found.
Here, we tell the HPC to use a newer version of the GNU compiler and load the recent R version, R 3.5.
Furthermore, we tell it to load zeromq, a messaging library, more details to that in section x.x.

The `.bashrc` is loaded when we login to the HPC, so every time we start R now, we start with the version we selected with the `module` command.
This is of particular interest, if you change between R versions - packages are installed for every version of R, so you have to be careful with that.

### 3. Create .profile

We have to do the same for a `.profile` file, as we did for the `.bashrc` file, so we create it:
```{bash eval = FALSE}
nano .profile
```

and copy and paste the following in it:

```{bash eval = FALSE}
## export LSF_SERVERDIR=/opt/lsf/10.1/linux2.6-glibc2.3-x86_64/etc
## export LSF_ENVDIR=/opt/lsf/conf
```

This ensures that after we login to the HPC, we find the commands such as `bsub` and `bjobs`.

### 4. Check linux shell

Login to [www.gwdg.de](www.gwdg.de) and check in your user settings that the linux shell you are usin is `/bin/ksh` and not `/bin/sh`.

## Use R on the GWDG HPC

If the previous steps all worked out, you should be able to login to HPC without typing your password.
If so, using R should be no problem now and

## GWDG HPC infrastructure

The HPC infrastructure is mostly accessible through queues.
A full overview can be found [http://hpc.gwdg.de/systems.html](here).

The most 

### bsub 

If you would use the HPC from the command line and without all the R packages,
you would most likely use the LSF command `bsub` to start and control your jobs.

If you follow this guide, we will use R functions as replacement for `bsub`.
However, to understand error messages and so on, it is important to remember that underneath, we still submit jobs via `bsub` only with R as proxy.

### LSF commands

The most important commands to look at what is happening on the HPC
are probably:

* `bjobs`
  * Shows you your job list and if your jobs are pending or running.
* `bkill`
  * Lets you kill jobs, for example if you did something wrong 
  * `bkill 0` kills every job in the current job list
* `bhist`
  * Most relevant as `bhist -a`, which shows you a history of your finished jobs. The runtime is interesting, as it gives you also a feeling for the walltime you can specify.

### modules

As described above, modules help you select software versions you need to run your code.
The module workflow on the HPC is rather straightforward, you look at the full list of modules:

```{bash eval = FALSE}
module avail
```

... load the ones you need, for example:

```{bash eval = FALSE}
module load R
```
... you can also look at the ones you already have loaded. This list should at least contain the ones you specified in the `.bashrc`.
```{bash eval = FALSE}
module list
```
And you can also detach the modules:
```{bash eval = FALSE}
module purge R
```
... or disconnect and connect again via ssh, which also removes all modules not specified in the `.bashrc`.

### Sequential jobs or exclusive nodes?

## R Packages

Always keep in mind that every package you use in your local scripts must also be installed on the HPC.
Therefore, login to the HPC and start R:

```{bash eval = FALSE}
R
```

(If you followed our instructions the R version should be R 3.5.0 now)

Installing R packages works as expected:

```{r eval = FALSE}
# For example:
install.packages("tidyverse")
```

If you use development versions from GitHub, you need to specify your local package path.
Therefore, you first need to install *withr*:
```{r eval = FALSE}
install.packages("withr")

# and look for your .libPath
.libPath()
```

Installing GitHub packages then works like this (you would have to exchange my local path with the one you printed with `.libPaths` and obviously the package you want):
```{r eval = FALSE}
withr::with_libpaths("/home/uni08/msciain/R/x86_64-pc-linux-gnu-library/3.5", devtools::install_github("r-spatialecogy/landscapemetrics")
```

## The future way

One of the ways to use the GWDG HPC is the future framework.
The advantage of future is that you most likely can ran the code you already have and only have to do minor changes so that it works on the HPC.
Furthermore, specifying how you want to parallelise your R code is quite straightforward, as you can easily control how to distribute your jobs
over nodes and cores.

Getting familiar with **future** makes probably some sense here, as we do not cover the basics:

[https://github.com/HenrikBengtsson/future](https://github.com/HenrikBengtsson/future)

The necessary steps to use future on the HPC include:

### 1. lsf.tmpl
Login to the HPC and create a file called `lsf.tmpl` in your home directory:

```{bash eval = FALSE}
nano lsf.tmpl
```

Copy and paste the following in it:

```{bash eval = FALSE}
## Default resources can be set in your .batchtools.conf.R by defining the variable
## 'default.resources' as a named list.

#BSUB -J <%= job.name %>                             ## Name of the job
#BSUB -o <%= log.file %>                             ## Output is sent to logfile, stdout + stderr by default
#BSUB -q <%= resources$queue %>                      ## Job queue
#BSUB -W <%= resources$walltime %>                   ## Walltime in minutes
#BSUB -R span[hosts=1]                               ## use only one host
#BSUB -n <%= resources$processes %>                  ## number of processes

## Export value of DEBUGME environemnt var to slave
## export DEBUGME=<%= Sys.getenv("DEBUGME") %>

module load gcc/6.3.0
module load R
module load R_GEO

Rscript -e 'batchtools::doJobCollection("<%= uri %>")'
```

This file is used to control how you submit jobs. 
It is a template that fills out the LSF `bsub` command, more details on that in the section with R code.

### 2. First login

*Remember that every package from now on needs to be installed on your computer and also on the HPC!*
The following code sends every future you use in your R code to the HPC, in the order you specify in your future plan:

```{r eval = FALSE}
# laod the packages
library("future")
library("future.batchtools")
library("furrr")

# now we specify a future topology that fits our HPC
# login node -> cluster nodes -> core/ multiple cores
login <- tweak(remote, workers = "gwdu101.gwdg.de", user = 'msciain') # local -> hpc, the user is your login credential

bsub <- tweak(batchtools_lsf, template = 'lsf.tmpl', 
              resources = list(job.name = 'params_nlmr',
                               log.file = 'params_nlmr.log',
                               queue = 'mpi-short',
                               walltime = '00:30', 
                               processes = 12)) # hpc -> nodes

plan(list(
  login,
  bsub,
  multisession # how to run on nodes, could also be sequential
))
```

### 3. Going down the future topology

We can now reach the **first level** of the HPC, the frontend, with:

```{r eval = FALSE}
# Before we start, despite that we have declared our future plan above,
# we are still working on our local machine if we do not use futures:
local_sysinfo <- Sys.info()
local_sysinfo

# To do something on the hpc, we actually have to use a future, 
# in the following example with the future assignment %<-%
hpc_sysinfo %<-% Sys.info()
hpc_sysinfo
```

We now executed the function `Sys.info` on the frontend of the HPC, where we land after logging in to gwdu101.gwdg.de.

The **second level**, the cluster nodes, can be reached if we nest a future in another future.
Here, we are using **furrr**, which lets you use the purrr::map family with futures:

```{r eval = FALSE}
# do something on the cluster node level
hpc_sysinfo_on_nodes  %<-%  future_map(seq_len(10), ~ Sys.info())
hpc_sysinfo_on_nodes
```

Now, we used `%<-%` as the first level of nesting and `future_map` as the second level of future nesting.
With respect to the HPC, this means that we logged in to the frontend, and distributed jobs over 10 cluster nodes.
Up to this level, we control how to distribute jobs to the HPC.
This means we always need at least this two nested future levels to submit jobs.
Note that the second level also always controls the total number of jobs you submit.
This becomes very important if we later think about sequential job submission and exclusive jobs with future.

The **third level**, the cores on each cluster node you address in the second level, the function you specify in the second level future.
For example, in our second level in the future plan, `bsub`, we said that we want to have 12 processes on each cluster node (more details in the next section).
The third level was a multisession, which in combination means that we block 12 cores on each cluster node we address.
Hence, our simulation could run on 12 cores and use the memory every core comes with.

```{r eval = FALSE}
# do something on core level of each cluster node
hpc_sysinfo_on_node_cores  %<-%  future_map(seq_len(10), function(x){
  future_map(seq_len(12), ~ Sys.info())
})
```

Here, we distributed our jobs over 10 nodes that had at least 12 available cores.
In total, we therefore executed 10 x 12 times `Sys.info()`.
`hpc_sysinfo_on_node_cores` is therefore a two dimensional list, with 10 elements where each of these has again 12 elements.
The 12 elements in each outer dimension of the list should be same, but differ between the unique outer elements overall.

### 4. future.batchtools lsf template

The future plans are crucial, as they specify how to distribute jobs.
Before submitting jobs to the HPC you therefore have to make sure, that the architecture of your plan and how your code is parallelised match together.
Otherwise, you misuse the scheduling software and drain ressources from other users.

The following code snippet explains how control the access specific queues, set the walltime and how to design a sequential plan:

```{r eval = FALSE}
# login node -> cluster nodes -> core/ multiple cores

# there shouldn't be to much change for the login level of your plan
# you specify the address of the hpc and your user name
login <- tweak(remote, workers = "gwdu101.gwdg.de", user = 'msciain')

# the bsub, or cluster node, level becomes the first stage
# where you could make adjustment to control you experiment.
# This tweak fills out the space between the curly brackets
# in the lsf.tmpl file we created on the HPC.
# (scroll to the right to read an explanation of every line)
bsub <- tweak(batchtools_lsf, template = 'lsf.tmpl',            # here we tell future.batchtools where to look for the template file we created in section
              resources = list(job.name = 'params_nlmr',        # name of your jobs in the LSF scheduling system
                               log.file = 'params_nlmr.log',    # future creates a hidden folder (next section), where it stores logfiles with this name for every job
                               queue = 'mpi-short',             # set the queue you plan to use
                               walltime = '00:30',              # set a walltime for your hobs
                               processes = 12))                 # block n cores on each node

plan(list(
  login,
  bsub,
  multisession # we specifically need multisession now to address the 12 cores we blocked on the bsub level
))
```

This setup makes a lot of sense if you have specific needs for your jobs.
This could either be that some of them depend of each other, you need a specific chip infrastructure or you have memory needs that are only satisfied if you block whole cluster nodes and run there exclusive.

However, we found another way to submit jobs quite useful.
This would be the case if you have a very high number of jobs, they do not depend on each other and each of them does not need more memory than you are allowed to use per core you use.
In this case, it makes sense to submit a high number of very small jobs, let's say you have a dataframe with 500 rows and each column there is an argument for a simulation function.
You could wait for example for 5 free cluster nodes and run there 100 jobs in parallel, however the time you would have to wait for that can be quite long.
Instead you can tweak your bsub level like this:


```{r eval = FALSE}
# (scroll to the right to read an explanation of every line)
bsub <- tweak(batchtools_lsf, template = 'lsf.tmpl',            # here we tell future.batchtools where to look for the template file we created in section
              resources = list(job.name = 'params_nlmr',        # name of your jobs in the LSF scheduling system
                               log.file = 'params_nlmr.log',    # future creates a hidden folder (next section), where it stores logfiles with this name for every job
                               queue = 'mpi-short',             # set the queue you plan to use
                               walltime = '00:30',              # set a walltime for your hobs
                               processes = 1))                  # block 1 cores on each node

plan(list(
  login,
  bsub,
  sequential # we need sequential here, so that every job we submit only runs on a single core
))
```

If we submit jobs now, the fill every every available slot on every node with free slots.
This means that sometimes we run several jobs on the same cluster node but they are also distributed over several cluster nodes.
This has the advantage that there are always free cores somewhere on the HPC and hence, the pending time of jobs is short.

### 5. .future folder

**future.batchtools** creates a folder called `.future/` in your home directory on the HPC. 
The jobs are there collected for each total submit in folders indicating the submit date and time.
There you find the logs, results etc of **failed** jobs.
If you succesfully collect the results on your local machine after finishing the jobs, the folder is empty and only contains a `.sessioninfo.txt`.

Note: You should remove failed jobs there, if they are not of need anymore. After a while, this folder can use up all your disk quota on the hpc.

### 6. Small example

A very basic demand one could probably have to the HPC:

- I need 120 cores to simulate jobs
- I need more than 10 GB of RAM
- My simulation takes about 12 hours, give or take

To run something like that, we would do:

```{r eval = FALSE}
# laod the packages
library("future")
library("future.batchtools")
library("furrr")
library("tidyverse")

# login node -> cluster nodes -> core/ multiple cores
login <- tweak(remote, workers = "gwdu101.gwdg.de", user = 'msciain')

# We submit to the multipurpose queue with a maximum
# walltime of 48 hours (we specify 15 so that we
# we aren not pending that long).
# We furthermore specify 24 processes, which
# automatically means that we reserve nodes
# exclusively (the maximum number of cores on
# mpi nodes is 24, which means if we say 24
# here we wait until a node is not used by anyone).
bsub <- tweak(batchtools_lsf, template = 'lsf.tmpl',            
              resources = list(job.name = 'params_nlmr',        
                               log.file = 'params_nlmr.log',    
                               queue = 'mpi',             
                               walltime = '15:00',              
                               processes = 24))                 

plan(list(
  login,
  bsub,
  multisession # Again: multisession to distribute over all the cores
))

# let's create an imagenary data frame to iterate over
y %<-% furrr::future_map_dfr(1:5, function(x){
  cores <- availableCores()
  furrr::future_map_dfr(1:5, function(x){
    tibble::tibble(x = x, 
                   nodename = Sys.info()['nodename'],
                   Sys.getpid = Sys.getpid(), 
                   availableCores = cores)
    })
  })
print(y)
```