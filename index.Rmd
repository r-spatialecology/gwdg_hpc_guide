---
title: "*GWDG* HPC FOR R DUMMIES"
author:
- Marco Sciaini
- Maximilian H.K. Hesselbarth
output:
 prettydoc::html_pretty:
   toc: true
   toc_depth: 2
   theme: architect
   highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# *GWDG* HPC

## Introduction
The aim of this guide is to explain how to use a local `R` session and submit jobs to the [*GWDG* High Performance Cluster](https://info.gwdg.de/docs/doku.php?id=en:services:application_services:high_performance_computing:start) (HPC) and retrieve the results in the same `R` session (requires *GWDG* account). The big advantage of this is that working locally in an IDE (e.g. [RStudio](https://www.rstudio.com)) is way more convenient than working with Linux shell on the HPC. Also, it is not necessaire
to manually copy data to the HPC (and vice versa). The structure of this guide is heavily influenced by our own mistakes and we hope to make it somewhat easier for future HPC user.

In general, our setup should also work with any other HPC and the code snippets should be usable for other scheduling systems than [**SLURM**](https://slurm.schedmd.com) (which is used by the *GWDG*) with slight modifications. To see the outdated version of this guide for [**LSF**](https://www.ibm.com/support/knowledgecenter/en/SSWRJV_10.1.0/lsf_welcome/lsf_welcome.html), there is a GitHub branch called *LSF* in [this repo](https://github.com/r-spatialecology/gwdg_hpc_guide).

## General setup

The user account must be activated to use the HPC. Therefore, an e-mail to [hpc@gwdg.de](mailto:hpc@gwdg.de) asking to activate the account must be sent.

### 1. Check Linux shell

The Linux shell must be set to `/bin/ksh` (and not `/bin/sh`). It's possible to set this in the user settings at [www.gwdg.de](www.gwdg.de).

### 2. SSH Key
For a convenient login and to use `R` via SSH connector, a SSH key is needed on the HPC (private key on local computer, public key on HPC).

Using *Linux* or *macOS*, it is straightforward to generate a SSH key and copy it to the HPC using the shell/terminal.

```{bash eval = FALSE}
ssh-keygen -t rsa
ssh-copy-id gwdg_user@gwdu101.gwdg.de
```

If `copy` is not installed by default, it can be added using the following command (*macOS*).

```{bash eval = FALSE}
brew install ssh-copy-id
```

For *Windows*, [this guide](https://docs.joyent.com/public-cloud/getting-started/ssh-keys/generating-an-ssh-key-manually/manually-generating-your-ssh-key-in-windows) seems to cover how to connect via SSH.

If the SSH key works, it should be possible to login to the HPC frontend without a password using the following command.

```{bash eval = FALSE}
ssh gwdg_user@gwdu101.gwdg.de
```

### 3. Create .profile

Because `.bashrc` is not source by Scientific Linux by default ([source](https://info.gwdg.de/docs/doku.php?id=en:services:application_services:high_performance_computing:bashrc)), firstly a `.profile` file is created. The following code enables that `.bashrc` is sourced everytime the HPC is accessed.

```{bash eval = FALSE}
nano .profile
```

```{bash eval = FALSE}
case "$SHELL" in
/bin/bash)
    . /etc/bashrc
    . .bashrc
    ;;
/usr/bin/bash)
    . /etc/bashrc
    . .bashrc
    ;;
/usr/local/bin/bash)
    . /etc/bashrc
    . .bashrc
    ;;
*)  ;;
esac
```

### 4. Create .bash-files

Now, the `.bashrc` file is sourced everytime the HPC is accessed. This is convenient because it allows to set default settings, load aliases or modules. To keep everything organized, within `.bashrc` again different files are sourced (if they are present). This includes `.bash_aliases` and `.bash_modules`.

```{bash eval = FALSE}
nano .bashrc
```

```{bash eval = FALSE}
# .bashrc
# Source global definitions
if [ -f /etc/bashrc ]; then
    . /etc/bashrc
fi

# load aliases
if [ -f ~/.bash_aliases ]; then
    . ~/.bash_aliases
fi

# load modules
if [ -f ~/.bash_modules ]; then
    . ~/.bash_modules
fi
```

Here are some convenient aliases that could be saved in the `.bash_aliases` file (created using `nano` as before). `.bash_aliases` makes sure they are available on the HPC and allow an easier application of common **SLURM** commands. Of course, this list can be expanded by any alias required. This does not only include **SLURM** commands, but can also include *Linux* commands.

```{bash eval = FALSE}
alias jobs_p='squeue'
alias jobs_med='squeue --partition=medium-fas,medium-fmz'
alias jobs_fat='squeue --partition=fat-fas,fat-fmz'
alias jobs_user='squeue -u $USER'
alias jobs_run='squeue -u $USER --states=RUNNING'
alias jobs_pen='squeue -u $USER --states=PENDING'
alias jobs_n='squeue -u $USER --states=RUNNING | wc -l'
alias jobs_kill='scancel -u $USER'
alias jobs_info='sacct -u $USER'
alias monitor='htop -u $USER'
```

To be able to automatically import modules, `.bash_modules` (again, using `nano`) needs to be created. Modules provide a way to selectively activate and deactivate modifications to the user environment. This allows particular packages and versions to be found. First, the *GNU* compiler `gcc` is loaded. Second, the  messaging library `zeromq` is loaded and used later to submit jobs to the HPC. Additionally, `R_GEO` allows to load certain spatial packages in `R` later. Of coures, it's possible to load any module which is needed (to see all available modules, use `module available`).

```{bash eval = FALSE}
module load gcc
module load zeromq
module load R_GEO
```

### 5. Try to run `R` on the HPC

If the previous steps all worked out, it should be possible to login to HPC without typing a password. If so, using `R` should be no problem now. The following command logs in to the HPC frontend and starts a `R` terminal.

```{bash eval = FALSE}
ssh username@gwdu101.gwdg.de
R
```

## *GWDG* HPC infrastructure

The HPC infrastructure is mostly accessible through partitions (formerly queues). A full overview can be found here: [High-Performance Computing in GÃ¶ttingen (outdated)](http://hpc.gwdg.de/systems.html). There are two general base partitions that are of interest.

* **medium** - lots of cores, fewer memory, short pending time (formerly MPI)
* **fat** - fewer cores, lots of memory, long pending time

Both partitions have additional a "quality of service" (QOS), i.e. "normal", "short" and "long" (formerly appended using "-"). This refers to the maximum walltime a job can run on the HPC. Of course this also relates to possible pending times. Generally, the possible pending time migth be longer in the "long" QOS. However, once a job is running, it's also allowed to use the partition longer.

More details can be found here.

* [RUNNING JOBS WITH SLURM](https://info.gwdg.de/dokuwiki/doku.php?id=en:services:application_services:high_performance_computing:running_jobs_slurm)
* [OUTDATED RUNNING JOBS (FOR EXPERIENCED USERS)](https://info.gwdg.de/dokuwiki/doku.php?id=en:services:application_services:high_performance_computing:running_jobs_for_experienced_users)

### sbatch command

The **SLURM** command `sbatch` is used to submit jobs to the HPC. This is important to remember to adjust submissions of jobs and understand error messages. Regular `R` syntax is only used as a proxy.

### General **SLURM** commands

Here are the previously defined aliases for the most important **SLURM** commands to monitor and control jobs on the HPC. Of course, it is possible to change this list by modifiying `.bash_aliases`. Also, it's possible to combine all aliases with the default **SLURM** options or just use the original **SLURM** commands.

* `sinfo`: General information about all partitions
* `jobs_p`/`jobs_med`/`jobs_fat`: All jobs submitted to all/medium/fat partition(s)
* `jobs_user`/`jobs_run`/`jobs_pen`: Only own all/running/pending submitted jobs
* `jobs_n`: Number of own running jobs. Because the header is counted, actual count is n - 1
* `jobs_kill`: Kill all submitted jobs
* `jobs_info`: Information about finished jobs

### Loading modules

As described above, modules help to select software versions needed to run code. The following command displays all available modules provided by the *GWDG*.

```{bash eval = FALSE}
module avail
```

Modules can be loaded using the following command and the name of the corresponding module.

```{bash eval = FALSE}
module load R
```

It is possible to list all loaded modules. This list should at least contain all modules specified in the `.bash_modules` file plus all later addtionally loaded modules.

```{bash eval = FALSE}
module list
```

Disconnecting and reconnecting via ssh also removes all modules (with exception of those specified in the `.bash_modules` file). To detach selected modules or all modules at once without logging out, the `purge` command can be used.

```{bash eval = FALSE}
module purge R
module purge
```

## Installing packages

All packages used in a local scripts must also be also installed on the HPC. This must be done for all `R` versions seperatly, i.e. if `R` is updated on the HPC, all packages must be re-installed. The easiest way to install packages on the HPC is to login to the frontend and start `R`.

Installing `R` packages works as expected. The only difference to installing packages locally is that `R` might asks to use the private library of the user (in which the user has writing rights).

```{r eval = FALSE}
# For example
install.packages("tidyverse")
```

To install packages from [GitHub](https://github.com/) it is necessaire to specify the path to the private library. Therefore, first the `devtools` and `withr` packages need to be installed. Now, the path can be provided where the package should be installed. It's possible to get this path using `.libPaths()`. This path is wrapped around the `install_github()` function.

```{r eval = FALSE}
install.packages("devtools")
install.packages("withr")

# and look for your .libPath
.libPaths()

withr::with_libpaths(new = "/home/uni08/user_name_gwdg/R/x86_64-pc-linux-gnu-library/3.5",
                     code = devtools::install_github("r-spatialecogy/landscapemetrics")
```

## Using the future package

One of the ways to use the *GWDG* HPC is the `future` package and framework. The advantage of `future` is that code can be run on the HPC with only minor changes. Furthermore, specifying how  to parallelise the `R` code is straightforward, as to control how to distribute jobs over nodes and cores. A [basic knowledge](https://github.com/HenrikBengtsson/future) of `future` is advised.

### 1. Create template to submit jobs

A template file is used to control how jobs are submitted to the HPC. This allows to pass arguments as `R` syntax from the local `R` session to the **SLURM** system as `sbatch` commands. The template file, called e.g. `future_slurm.tmpl`, needs to be created on the HPC (same same `nano future_slurm.tmpl`). The follwing is an example for such a template file allowing to specify the most important `sbatch` options.

```{bash eval = FALSE}
## Default resources can be set in your .batchtools.conf.R by defining the variable
## 'default.resources' as a named list.

#!/bin/sh
#SBATCH -J <%= job.name %>                             ## Name of the job
#SBATCH -p <%= resources$queue %>                      ## Job queue
#SBATCH -q <%= resources$service %>                    ## QOS
#SBATCH -t <%= resources$walltime %>                   ## Walltime in minutes
#SBATCH -n <%= resources$processes %>                  ## number of processes
#SBATCH -N 1				                                   ## use only one host
#SBATCH -o <%= log.file %>                             ## Output is sent to logfile

## Export value of DEBUGME environemnt var to slave
## export DEBUGME=<%= Sys.getenv("DEBUGME") %>

module load gcc
module load R_GEO

Rscript -e 'batchtools::doJobCollection("<%= uri %>")'
```

### 2. First login

**Every package from now on needs to be installed on both the local computer and the HPC!**

The following `R` code sends every `future` to the HPC, as specified by the `future` plan.

```{r eval = FALSE}

# load the packages
library("future.batchtools")
library("future")
library("furrr")

# now we specify a future topology that fits our HPC
# login node -> cluster nodes -> core/ multiple cores
login <- tweak(remote, workers = "gwdu101.gwdg.de", user = 'user_name_gwdg') # user = login credential

sbatch <- tweak(batchtools_slurm, template = 'future_slurm.tmpl',
                resources = list(job.name = 'run_hpc',     # name of the job
                                 log.file = 'run_hpc.log', # name of log file
                                 queue = 'medium', # which partition
                                 service = "short" # which QOS
                                 walltime = '00:05:00', # walltime <hh:mm:ss>
                                 processes = 12)) # number of cores

plan(list(
  login,
  sbatch,
  multisession # how to run on nodes, could also be sequential
))
```

### 3. Going down the future topology

Now it's possible to reach the **first level** of the HPC (frontend). After logging in to *gwdu101.gwdg.de*, the function `Sys.info()` is executed on the frontend of the HPC.

```{r eval = FALSE}
# Before we start, despite that we have declared our future plan above,
# we are still working on our local machine if we do not use futures:
local_sysinfo <- Sys.info()

local_sysinfo

# To do something on the hpc, we actually have to use a future,
# in the following example with the future assignment %<-%
hpc_sysinfo %<-% Sys.info()

hpc_sysinfo
```

The **second level**, the cluster nodes, can be reached  with a nested `future`. The `furrr` packages allows to use the `purrr::map`-family as `futures`. The `future`-operator `%<-%` is used to accesses the HPC frontend and `future_map` to accesses the second level cluster nodes. Because code should never be executed on the frontend, these two nested `future` levels are always required to submit jobs. The second level also controls the total number of jobs submitted. This becomes very important later for sequential job submission and exclusive jobs with `future`.

```{r eval = FALSE}
# do something on the cluster node level
hpc_sysinfo_on_nodes  %<-%  future_map(seq_len(10), ~ Sys.info())

hpc_sysinfo_on_nodes
```

The **third level**, the cores on each cluster node, is adressed by the settings specified in the second level. For example, in the second level `multisession` with `processes = 12` can specified. This means demanding 12 cores on each cluster node and run the function parallel on each. Hence, the function could run on 12 cores and use the memory every core comes with. Here, the jobs are distributed over 10 nodes that have at least 12 available cores. In total, `Sys.info()` is executed 10 x 12 times. Therefore, `hpc_sysinfo_on_node_cores` is a two dimensional list, with 10 elements where each of these has again 12 elements. The 12 elements in each outer dimension of the list should be the same, but differ between the unique outer elements overall.

```{r eval = FALSE}
# do something on core level of each cluster node
hpc_sysinfo_on_node_cores  %<-%  future_map(seq_len(10), function(x){
  future_map(seq_len(12), ~ Sys.info())
})
```

### 4. Submitting jobs efficiently 

The `future` plans are crucial, as they specify how to distribute jobs. Therefore, before submitting jobs to the HPC it is important to ensure that the architecture of the plan and how the code is parallelised match. Otherwise, it may be possible to bypass the scheduling system and drain ressources from other users.

The following code snippet explains how to access specific partitions, set the walltime and design a sequential plan. This setup makes sense if the jobs depend of each other, need a specific chip infrastructure or have memory needs that are only satisfied if whole cluster nodes are blocked and jobs are run there exclusively.

```{r eval = FALSE}
# login node -> cluster nodes -> core/ multiple cores

# there shouldn't be to much change for the login level of your plan
# you specify the address of the hpc and your user name
login <- tweak(remote, workers = "gwdu101.gwdg.de", user = 'user_name_gwdg')

# the sbatch, or cluster node, level becomes the first stage
# where you could make adjustment to control you experiment.
# This tweak fills out the space between the curly brackets
# in the future_slurm.tmpl file we created on the HPC.
# (scroll to the right to read an explanation of every line)
sbatch <- tweak(batchtools_slurm, template = 'future_slurm.tmpl',
                resources = list(job.name = 'run_hpc',     # name of the job
                                 log.file = 'run_hpc.log', # name of log file
                                 queue = 'medium', # which partition
                                 service = "short" # which QOS
                                 walltime = '00:05:00', # walltime <hh:mm:ss>
                                 processes = 12)) # number of cores

plan(list(
  login,
  sbatch,
  multisession # multisession because 12 cores were blocked on the sbatch level
))
```

However, if jobs are independent of each other (e.g. repetitions of the same function) and do not need more memory than one core provides, it makes sense to submit a high number of very small jobs. Instead of blocking a whole node and running the function on all availaible nodes in parallel (submitted as one job), it is also possible to submit 12 single jobs only demanding one core per job. This has the advantage that the pending time will be decreased enormously. The reason for this is that the jobs will be submitted to any available free slot (1 core) on the cluster. The **SLURM** system automatically schedules these small jobs with a higher priority in order to use the capacities optimally.

```{r eval = FALSE}
# (scroll to the right to read an explanation of every line)
sbatch <- tweak(batchtools_slurm, template = 'future_slurm.tmpl',
                resources = list(job.name = 'run_hpc',     # name of the job
                                 log.file = 'run_hpc.log', # name of log file
                                 queue = 'medium', # which partition
                                 service = "short" # which QOS
                                 walltime = '00:05:00', # walltime <hh:mm:ss>
                                 processes = 1)) # number of cores

plan(list(
  login,
  sbatch,
  sequential # we need sequential here, so that every job we submit only runs on a single core
))
```

### 5. .future folder

`future.batchtools` creates a folder called `.future/` on the HPC in the home directory. In this folder all submitted jobs are collected and the folder structure indicates the date and time of submission. If all jobs are collected succesfully and retrieved on the local computer, the folder is empty and only contains a `.sessioninfo.txt`. However, all logs and (partial) results of failed jobs won't be deleted. This also includes jobs that are killed by the user. 

**Note**: It's advisable to remove failed jobs from time to time in the `.future/` folder. After a while, this folder can use up a lot of the disk quota on the HPC.

### 6. Example

Submit jobs to the HPC with the following requirements: 

* 120 cores to simulate jobs
* More than 10 GB of RAM
* Functions need about 12 hours to run

```{r eval = FALSE}
# laod the packages
library("future")
library("furrr")
library("tidyverse")

# login node -> cluster nodes -> core/ multiple cores
login <- tweak(remote, workers = "gwdu101.gwdg.de", user = 'username')

# We submit to the multipurpose queue with a maximum
# walltime of 48 hours (we specify 15 so that we
# we aren not pending that long).
# We furthermore specify 24 processes, which
# automatically means that we reserve nodes
# exclusively (the maximum number of cores on
# mpi nodes is 24, which means if we say 24
# here we wait until a node is not used by anyone).
sbatch <- tweak(batchtools_slurm, template = 'future_slurm.tmpl',
                resources = list(job.name = 'run_hpc',     # name of the job
                                 log.file = 'run_hpc.log', # name of log file
                                 queue = 'medium', # which partition
                                 service = "normal" # which QOS
                                 walltime = '00:15:00', # walltime <hh:mm:ss>
                                 processes = 12)) # number of cores

plan(list(
  login,
  sbatch,
  multisession # Again: multisession to distribute over all the cores
))

# let's create an imagenary data frame to iterate over
mydata <- data.frame(x = rnorm(100), y = rnorm(100))
advanced_data <- list(mydata, mydata, mydata, mydata, mydata)

fancy_statistical_model <- future_map_dfr(advanced_data, function(x) {
  future_map_dfr(seq_len(ncol(x)), function(y) {
    single_row <- x[y, ]
    tibble(single_row$x + single_row$y)
  }, .id = "y")
}, .id = "x")
```

## Using the clustermq package

`clustermq` is another package that allows to submit jobs to the HPC via SSH connector. Compared to `future` (and "friends"), it has the advantage that it submits job arrays. Instead of gradually sending job after job, as done in the examples before, `clustermq` sends them all at once. This can be a huge time benefit.

First, all necessary dependencies need to be installed on the local system. To use `clustermq`, it has to be installed on both the local computer and the HPC.

```{bash eval=FALSE}
# You can skip this step on Windows and OS-X, the rzmq binary has it
# On a computing cluster, we recommend to use Conda or Linuxbrew
brew install zeromq # Linuxbrew, Homebrew on OS-X
conda install zeromq # Conda
sudo apt-get install libzmq3-dev # Ubuntu
sudo yum install zeromq3-devel # Fedora
pacman -S zeromq # Archlinux
```

```{r eval=FALSE}
# local pc
devtools::install_github('mschubert/clustermq')

# HPC
withr::with_libpaths("/home/uni08/user_name_gwdg/R/x86_64-pc-linux-gnu-library/3.5", devtools::install_github('mschubert/clustermq'))
```

Afterwards, the local `.Rprofile` needs to be modified. This can be done with the `usethis` package.

```{r eval=FALSE}
edit_r_profile()
```

```{r eval=FALSE}
options(
    clustermq.scheduler = "ssh",
    clustermq.ssh.host = "username@gwdu101.gwdg.de", # use your user and host, obviously
    clustermq.ssh.log = "~/cmq_ssh.log" # log for easier debugging
)
```

Also the `.Rprofile` on the HPC (`nano .Rprofile` in the home directory) needs to be modified.

```{r eval=FALSE}
options(
    clustermq.scheduler = "slurm",
    clustermq.template = "/home/uni08/username/clustermq_slurm.tmpl"
)
```

Then, again an template to submit jobs is needed, e.g. `nano clustermq_slurm.tmpl`. The template submits jobs to the HPC specifying the options the scheduling system uses.

```{bash eval=FALSE}
#!/bin/sh
#SBATCH --job-name={{ job_name }} # job name
#SBATCH --partition={{ queue | medium }} # name of queue
#SBATCH --qos={{ service | normal }} # which special QOS (short/long)
#SBATCH --time={{ walltime | 12:00:00 }} # walltime
#SBATCH --array=1-{{ n_jobs }} # number of processes
#SBATCH --nodes={{ nodes | 1 }} # if 1 put load on one node
#SBATCH --output={{ log_file | /dev/null }}
#SBATCH --error={{ log_file | /dev/null }}

module load gcc
module load zeromq
module load R_GEO

R --no-save --no-restore -e 'clustermq:::worker("{{ master }}")'
```

1. ### How to use clustermq

`clustermq` does not rely on plans as `future`. Submitting jobs becomes as easy as providing a function and a list or vector to iterate over. In the example, for each `x = 1...5` a single job is submitted to the HPC. Contrastingly, `y = 10` is hold constant for all submitted jobs. In case user-defined functions are needed, these can be specified by the the `export` argument of the `Q()` function. It's also possible to iterate over rows of a data frame.

```{r eval=FALSE}
library(clustermq)

fx = function(x, y) x * 2 + y
Q(fx, x = 1:5, const = list(y = 10), n_jobs = 1)
```

```{r eval=FALSE}
# Run a simple multiplication for data frame columns x and y on a worker node
fx = function (x, y) x * y

df = data.frame(x = 5, y = 10)

Q_rows(df, fx, job_size = 1)

# Q_rows also matches the names of a data frame with the function arguments
fx = function (x, y) x - y

df = data.frame(y = 5, x = 10)

Q_rows(df, fx, job_size = 1)
```

All parameters within ``{{ }}`` in the template file can be specified using a list and the `template` argument of the `Q()` function. Therefore, it is very easy to e.g. submit to the short *QOS* and set the walltime to 5 minutes (<hh:mm:ss>). Of course, additional arguments can be added to the template.

```{r eval=FALSE}
Q(fx, x = 1:3, const = list(y = 10), n_jobs = 3, 
  template = list(walltime = "00:05:00", 
                  service = "short"))
```

## Tips and Tricks

### 1.Kill jobs/R sessions

There is an alias to kill already submitted jobs. In case something goes wrong while submitting jobs, there will be most likely several `R` sessions that do not close by themselfes or are open and keep submitting jobs. A convienent way to kill these processes is to login to the HPC frontend and use the defined alias `monitor` to enter the process manger `htop`. With `F9` it's possible to select the `R` processes and kill them. `F10` closes the process manager afterwards.

```{bash eval = FALSE}
jobs_kill
monitor
```

### 2. Better connection to the HPC

One inconvience is that the connection to HPC from a local `R` session can not be interrupted. If that happens, jobs are lost (using `future` there might be chance to collect every result in the `.future` folder). This also means that running code from a local computer means that it has to be running until everything is finished. When code take several days, this can easily become a problem.

A way we explored for such cases is to use the [GWDG Cloud Server](https://www.gwdg.de/de/server-services/gwdg-cloud-server). If you install there an instance of RStudio Server, you have a cloud based IDE that you can run from everywhere with access to the internet. By default, the RStudio Server unfortunately breaks the connection every now and then, so you have to tweak it a bit to be permanently accessible. To turn off the suspended session feature, must be specified in a `ression.conf` file. After the line `session-timeout-minutes=0` is added to the file, the server must be restarted.

```{bash eval = FALSE}
cd /etc/rstudion
nano rsession.conf # add line session-timeout-minutes=0
```

```{bash eval = FALSE}
session-timeout-minutes=0
```

```{bash eval = FALSE}
sudo rstudio-server restart
```

Also the SSH connection becomes a bit tricky from the server. After the SSH pairs were added to the server and the HPC, it is first required to login to a *GWDG* login terminal and from there to the HPC frontend.

```{bash, eval = FALSE}
ssh username@login.gwdg.de
ssh username@gwdu101.gwdg.de
```

If this works, automate the two steps adding a `config` file (`nano config`) in the `.ssh/` folder on the HPC.

```{bash, eval = FALSE}
Host gwdu101.gwdg.de
  ProxyCommand ssh username@login.gwdg.de -W %h:%p
  StrictHostKeyChecking no
  ServerAliveInterval 30
```

In case you use another RStudio Server, the only thing you would have to change from the things we explained here is to open a VPN connection to the *goenet*.

## Links

* [https://github.com/HenrikBengtsson/future](https://github.com/HenrikBengtsson/future)
* [https://github.com/HenrikBengtsson/future.batchtools](https://github.com/HenrikBengtsson/future.batchtools)
* [https://github.com/mllg/batchtools](https://github.com/mllg/batchtools)
* [https://github.com/DavisVaughan/furrr](https://github.com/DavisVaughan/furrr)
* [https://github.com/mschubert/clustermq](https://github.com/mschubert/clustermq)

## Acknowledgments
Huge thanks to the GWDG HPC team that put quite some effort into installing `R` packages and explaining some HPC basics to us.
Another big thanks to [Henrik Bengtsson](https://twitter.com/henrikbengtsson?lang=en), [Michael Schubert](https://twitter.com/_ms03?lang=en) and [Michel Lang](https://github.com/mllg) for developing nice tools (future, clustermq, batchtools) to enable even ecologists to use cluster interfaces `r emo::ji("smile")`.